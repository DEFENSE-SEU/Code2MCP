{
  "repository": {
    "name": "TextBlob",
    "url": "https://github.com/sloria/TextBlob",
    "local_path": "E:\\code\\fastMCP\\fastMCP\\mcp-repo-output\\workspace\\TextBlob",
    "description": "Python library",
    "features": "Basic functionality",
    "tech_stack": "Python",
    "stars": 0,
    "forks": 0,
    "language": "Python",
    "last_updated": "",
    "complexity": "medium",
    "intrusiveness_risk": "low"
  },
  "execution": {
    "start_time": 1757485018.320429,
    "end_time": 1757486264.2249267,
    "duration": 1245.9044976234436,
    "status": "success",
    "workflow_status": "success",
    "nodes_executed": [
      "download",
      "analysis",
      "env",
      "generate",
      "run",
      "review",
      "finalize"
    ],
    "total_files_processed": 2,
    "environment_type": "conda",
    "llm_calls": 0,
    "deepwiki_calls": 0
  },
  "tests": {
    "original_project": {
      "passed": true,
      "details": {
        "passed": true,
        "report_path": null
      },
      "test_coverage": "100%",
      "execution_time": 0,
      "test_files": []
    },
    "mcp_plugin": {
      "passed": true,
      "details": {
        "passed": true,
        "report_path": null,
        "stdout": "",
        "stderr": "b_service                       │\n\n│                 📦 Transport:       STDIO                                  │\n\n│                                                                            │\n\n│                 🏎️  FastMCP version: 2.12.2                                 │\n\n│                 🤝 MCP SDK version: 1.13.1                                 │\n\n│                                                                            │\n\n│                 📚 Docs:            https://gofastmcp.com                  │\n\n│                 🚀 Deploy:          https://fastmcp.cloud                  │\n\n│                                                                            │\n\n└────────────────────────────────────────────────────────────────────────────┘\n\n\n\n\n\n[09/10/25 14:37:43] INFO     Starting MCP server                 server.py:1493\n\n                             'textblob_service' with transport                 \n\n                             'stdio'                                           \n\n\n"
      },
      "service_health": "healthy",
      "startup_time": 0,
      "transport_mode": "stdio",
      "fastmcp_version": "unknown",
      "mcp_version": "unknown"
    }
  },
  "analysis": {
    "structure": {
      "packages": [
        "source.src.textblob",
        "source.tests"
      ]
    },
    "dependencies": {
      "has_environment_yml": false,
      "has_requirements_txt": false,
      "pyproject": true,
      "setup_cfg": false,
      "setup_py": false
    },
    "entry_points": {
      "imports": [],
      "cli": [],
      "modules": []
    },
    "risk_assessment": {
      "import_feasibility": 0.9,
      "intrusiveness_risk": "low",
      "complexity": "medium"
    },
    "deepwiki_analysis": {
      "repo_url": "https://github.com/sloria/TextBlob",
      "repo_name": "TextBlob",
      "content": "sloria/TextBlob\nInstallation and Setup\nQuick Start Guide\nProject History and Evolution\nCore Architecture\nTextBlob Class\nWord and WordList Classes\nSentence Class\nBlobber Factory\nNLP Components\nPart-of-Speech Taggers\nNoun Phrase Extractors\nSentiment Analysis\nInflection and Word Manipulation\nAdvanced Features\nClassification System\nText Formats and Parsing\nCustomization and Extensions\nDevelopment Guide\nContributing Guidelines\nTesting Framework\nDocumentation System\nCHANGELOG.rst\ntests/test_blob.py\nTextBlob is a Python library for processing textual data that provides a simple API for common natural language processing (NLP) tasks. It serves as an accessible interface to powerful NLP capabilities without requiring deep expertise in computational linguistics. TextBlob builds upon the robust foundations ofNLTKandpatternlibraries, offering a more intuitive and streamlined experience for developers.\nThis document provides an overview of the TextBlob library's architecture, components, and design philosophy. For installation instructions and quick start examples, seeInstallation and SetupandQuick Start Guide.\nCore Architecture\nTextBlob is built around a small set of core classes that provide a rich interface for text processing operations. The main entry point for users is theTextBlobclass, which provides methods for various NLP operations.\nClass Hierarchy Diagram\n\"contains\"\"contains\"\"contains\"\"contains\"\"creates\"BaseBlob+text: str+tokenizer+pos_tagger+np_extractor+analyzer+parser+classifierTextBlob+sentences+words+tags+noun_phrases+sentiment+classify()+correct()+parse()+ngrams()Word+string: str+pos_tag: str+definitions+synsets+singularize()+pluralize()+lemmatize()+spellcheck()+correct()WordList+words: list+count()+pluralize()+singularize()+lemmatize()Sentence+string: str+start_index: int+end_index: int+words+tags+sentiment+parse()Blobber+tokenizer+pos_tagger+np_extractor+analyzer+parser+classifier+call(text)\n+pos_tagger\n+np_extractor\n+classifier\n+noun_phrases\n+classify()\n+string: str\n+pos_tag: str\n+definitions\n+singularize()\n+pluralize()\n+lemmatize()\n+spellcheck()\n+words: list\n+pluralize()\n+singularize()\n+lemmatize()\n+string: str\n+start_index: int\n+end_index: int\n+pos_tagger\n+np_extractor\n+classifier\n+call(text)\nThe architecture follows a hierarchical model:\nBaseBlob: The abstract base class that provides common functionality for text processing.\nTextBlob: The main class users interact with, extending BaseBlob with additional functionality.\nSentence: Represents a single sentence within a TextBlob.\nWord: Represents a single word with methods for inflection, lemmatization, etc.\nWordList: A collection of Word objects with methods for plural/singular forms and more.\nBlobber: A factory class for creating TextBlob instances with consistent configurations.\nSources:tests/test_blob.py231-252tests/test_blob.py851-951tests/test_blob.py968-1017\nNLP Pipeline\nTextBlob processes text through a pipeline of operations, with each step handling a specific NLP task. The output of one operation often serves as input to subsequent operations, creating a flexible processing chain.\nTextBlob Processing Pipeline Diagram\nNLP ComponentsTextBlob NLP PipelineRaw TextTextBlobTokenizationWordsSentencesPart-of-Speech TaggingNoun Phrase ExtractionSentiment AnalysisParsingClassificationWord InflectionSpelling CorrectionWordNet IntegrationPluggable ComponentsTokenizersPOS TaggersNP ExtractorsSentiment AnalyzersParsersClassifiersProcessing Results\nNLP Components\nTextBlob NLP Pipeline\nTokenization\nPart-of-Speech Tagging\nNoun Phrase Extraction\nSentiment Analysis\nClassification\nWord Inflection\nSpelling Correction\nWordNet Integration\nPluggable Components\nPOS Taggers\nNP Extractors\nSentiment Analyzers\nClassifiers\nProcessing Results\nKey operations in the pipeline include:\nTokenization: Breaking text into words and sentences using a tokenizer.\nPart-of-Speech Tagging: Identifying grammatical parts of speech for each word.\nNoun Phrase Extraction: Identifying noun phrases based on patterns of POS tags.\nSentiment Analysis: Calculating polarity (positive/negative) and subjectivity of text.\nParsing: Creating syntax trees for sentences.\nClassification: Categorizing text into predefined classes.\nWord Operations: Inflection, lemmatization, spelling correction, and WordNet lookups.\nEach of these operations is backed by pluggable components that can be customized or replaced.\nSources:tests/test_blob.py390-407tests/test_blob.py419-436tests/test_blob.py595-596tests/test_blob.py665-673\nComponent System\nTextBlob implements a pluggable component architecture where each NLP task has a base interface and multiple concrete implementations. This design enables flexibility and extensibility.\nComponent Implementation Diagram\n«interface»BaseTagger+tag(text)«interface»BaseNPExtractor+extract(text, tags)«interface»BaseTokenizer+tokenize(text)+itokenize(text)«interface»BaseSentimentAnalyzer+analyze(text)«interface»BaseParser+parse(text)«interface»BaseClassifier+classify(text)+train(data)NLTKTaggerPatternTaggerFastNPExtractorConllExtractorWordTokenizerSentenceTokenizerPatternAnalyzerNaiveBayesAnalyzerNLTKClassifierNaiveBayesClassifierDecisionTreeClassifierMaxEntClassifierPositiveNaiveBayesClassifier\n«interface»\n«interface»\nBaseNPExtractor\n+extract(text, tags)\n«interface»\nBaseTokenizer\n+tokenize(text)\n+itokenize(text)\n«interface»\nBaseSentimentAnalyzer\n+analyze(text)\n«interface»\n+parse(text)\n«interface»\nBaseClassifier\n+classify(text)\n+train(data)\nPatternTagger\nFastNPExtractor\nConllExtractor\nWordTokenizer\nSentenceTokenizer\nPatternAnalyzer\nNaiveBayesAnalyzer\nNLTKClassifier\nNaiveBayesClassifier\nDecisionTreeClassifier\nMaxEntClassifier\nPositiveNaiveBayesClassifier\nThe component system includes:\nTokenizers:WordTokenizer,SentenceTokenizer- For breaking text into units.\nWordTokenizer\nSentenceTokenizer\nPOS Taggers:NLTKTagger,PatternTagger- For assigning parts of speech.\nPatternTagger\nNP Extractors:FastNPExtractor,ConllExtractor- For extracting noun phrases.\nFastNPExtractor\nConllExtractor\nSentiment Analyzers:PatternAnalyzer,NaiveBayesAnalyzer- For analyzing sentiment.\nPatternAnalyzer\nNaiveBayesAnalyzer\nParsers:PatternParser- For parsing sentence structure.\nPatternParser\nClassifiers: Various implementations for text classification.\nEach component can be used independently or as part of the TextBlob pipeline, and users can provide their own implementations.\nSources:tests/test_blob.py15-18tests/test_blob.py485-498tests/test_blob.py470-473tests/test_blob.py472-483tests/test_blob.py452-468\nDefault Components\nTextBlob comes with default implementations for each component type. These defaults provide a good balance between accuracy and performance for general use cases.\nWordTokenizer\nFastNPExtractor\nPatternAnalyzer\nPatternParser\nSources:tests/test_blob.py731-733tests/test_blob.py485-487tests/test_blob.py452-456\nExtension and Customization\nTextBlob is designed to be highly customizable, allowing users to extend its functionality or replace components with their own implementations.\nCustomization Diagram\nExtensionsCore Systemtextblob-somethingtextblob-xxCustom ComponentsCustom TokenizerCustom TaggerCustom NP ExtractorCustom AnalyzerCustom ParserCustom ClassifierUser CodeTextBlobDefault NLP ComponentsWordTokenizerNLTKTaggerFastNPExtractorPatternAnalyzerPatternParserModel ExtensionsLanguage ExtensionsCustomComponents\nCore System\ntextblob-something\ntextblob-xx\nCustom Components\nCustom Tokenizer\nCustom Tagger\nCustom NP Extractor\nCustom Analyzer\nCustom Parser\nCustom Classifier\nDefault NLP Components\nWordTokenizer\nFastNPExtractor\nPatternAnalyzer\nPatternParser\nModel Extensions\nLanguage Extensions\nCustomComponents\nThere are several ways to customize TextBlob:\nComponent Injection: Specify custom components when creating a TextBlob or Blobber instance:\nfromtextblobimportTextBlobfromtextblob.tokenizersimportSentenceTokenizerfromtextblob.taggersimportNLTKTagger# Create a TextBlob with custom componentsblob = TextBlob(\"Some text\", tokenizer=SentenceTokenizer(), pos_tagger=NLTKTagger())\nfromtextblobimportTextBlobfromtextblob.tokenizersimportSentenceTokenizerfromtextblob.taggersimportNLTKTagger# Create a TextBlob with custom componentsblob = TextBlob(\"Some text\", tokenizer=SentenceTokenizer(), pos_tagger=NLTKTagger())\nfromtextblobimportTextBlobfromtextblob.tokenizersimportSentenceTokenizerfromtextblob.taggersimportNLTKTagger# Create a TextBlob with custom componentsblob = TextBlob(\"Some text\", tokenizer=SentenceTokenizer(), pos_tagger=NLTKTagger())\nBlobber Factory: Create a Blobber with custom components for consistent TextBlob creation:\nfromtextblobimportBlobberfromtextblob.tokenizersimportSentenceTokenizerfromtextblob.np_extractorsimportConllExtractor# Create a Blobber with custom componentstb = Blobber(tokenizer=SentenceTokenizer(), np_extractor=ConllExtractor())# Create TextBlobs with the same configurationblob1 = tb(\"Some text\")blob2 = tb(\"More text\")\nfromtextblobimportBlobberfromtextblob.tokenizersimportSentenceTokenizerfromtextblob.np_extractorsimportConllExtractor# Create a Blobber with custom componentstb = Blobber(tokenizer=SentenceTokenizer(), np_extractor=ConllExtractor())# Create TextBlobs with the same configurationblob1 = tb(\"Some text\")blob2 = tb(\"More text\")\nfromtextblobimportBlobberfromtextblob.tokenizersimportSentenceTokenizerfromtextblob.np_extractorsimportConllExtractor# Create a Blobber with custom componentstb = Blobber(tokenizer=SentenceTokenizer(), np_extractor=ConllExtractor())# Create TextBlobs with the same configurationblob1 = tb(\"Some text\")blob2 = tb(\"More text\")\nExtension Packages: Create extension packages for new models or language support.\nSources:tests/test_blob.py995-1004tests/test_blob.py1006-1011tests/test_blob.py1013-1016\nRecent Changes and Version Compatibility\nTextBlob is continually evolving. Recent versions have made several significant changes:\nRemoved translation functionality (use the official Google Translate API instead)\nSupport for Python 3.9-3.13 and nltk>=3.9\nRemoved vendorized modules\nPerformance improvements in text processing\nTextBlob maintains backward compatibility for core functionality while evolving to support newer Python versions and improve performance.\nSources:CHANGELOG.rst4-16CHANGELOG.rst30-32\nTextBlob provides a simple yet powerful interface for NLP tasks, making advanced text processing accessible to Python developers. Its modular architecture, pluggable components, and extensibility make it adaptable to a wide range of use cases, from simple sentiment analysis to more complex text classification and processing tasks.\nFor more detailed information on specific components and usage patterns, refer to the other sections of this documentation.\nSources:README.rst16-17README.rst50-64\nRefresh this wiki\nOn this page\nCore Architecture\nClass Hierarchy Diagram\nNLP Pipeline\nTextBlob Processing Pipeline Diagram\nComponent System\nComponent Implementation Diagram",
      "model": "gpt-5",
      "source": "selenium",
      "success": true
    },
    "code_complexity": {
      "cyclomatic_complexity": "medium",
      "cognitive_complexity": "medium",
      "maintainability_index": 75
    },
    "security_analysis": {
      "vulnerabilities_found": 0,
      "security_score": 85,
      "recommendations": []
    }
  },
  "plugin_generation": {
    "files_created": [
      "mcp_output/start_mcp.py",
      "mcp_output/mcp_plugin/__init__.py",
      "mcp_output/mcp_plugin/mcp_service.py",
      "mcp_output/mcp_plugin/adapter.py",
      "mcp_output/mcp_plugin/main.py",
      "mcp_output/requirements.txt",
      "mcp_output/README_MCP.md",
      "mcp_output/tests_mcp/test_mcp_basic.py"
    ],
    "main_entry": "start_mcp.py",
    "requirements": [
      "fastmcp>=0.1.0",
      "pydantic>=2.0.0"
    ],
    "readme_path": "E:\\code\\fastMCP\\fastMCP\\mcp-repo-output\\workspace\\TextBlob\\mcp_output\\README_MCP.md",
    "adapter_mode": "import",
    "total_lines_of_code": 0,
    "generated_files_size": 0,
    "tool_endpoints": 0,
    "supported_features": [
      "Basic functionality"
    ],
    "generated_tools": [
      "Basic tools",
      "Health check tools",
      "Version info tools"
    ]
  },
  "code_review": {},
  "errors": [],
  "warnings": [],
  "recommendations": [
    "Improve test coverage by adding more edge cases and scenarios",
    "Ensure all test reports are properly generated and stored",
    "Address the lack of stdout and stderr clarity in plugin tests",
    "Optimize dependency management by adding a requirements.txt file",
    "Validate compatibility with Python versions mentioned in the analysis",
    "Enhance documentation for MCP plugin usage and setup",
    "Refactor code to reduce complexity in core modules",
    "Implement performance benchmarks for key NLP components",
    "Add integration tests for MCP plugin endpoints",
    "Ensure proper error handling and logging in MCP services",
    "Update README files to include recent changes and version compatibility",
    "Verify the accuracy of sentiment analysis and classification components",
    "Improve modularity and extensibility of NLP pipeline components",
    "Conduct a risk assessment for dependency updates",
    "Streamline the import strategy for better maintainability."
  ],
  "performance_metrics": {
    "memory_usage_mb": 0,
    "cpu_usage_percent": 0,
    "response_time_ms": 0,
    "throughput_requests_per_second": 0
  },
  "deployment_info": {
    "supported_platforms": [
      "Linux",
      "Windows",
      "macOS"
    ],
    "python_versions": [
      "3.8",
      "3.9",
      "3.10",
      "3.11",
      "3.12"
    ],
    "deployment_methods": [
      "Docker",
      "pip",
      "conda"
    ],
    "monitoring_support": true,
    "logging_configuration": "structured"
  },
  "execution_analysis": {
    "success_factors": [
      "Workflow execution completed"
    ],
    "failure_reasons": [],
    "overall_assessment": "good"
  },
  "technical_quality": {
    "code_quality_score": 80,
    "architecture_score": 75,
    "performance_score": 70,
    "maintainability_score": 75
  }
}